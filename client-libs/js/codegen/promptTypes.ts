/**
 * This type map defines the input types for each model provider.
 */

export interface PromptTypes {
  "openai/ChatCompletion"?: {
    /**
     * ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API.
     */
    model:
      | "gpt-4"
      | "gpt-4-0613"
      | "gpt-4-32k"
      | "gpt-4-32k-0613"
      | "gpt-3.5-turbo"
      | "gpt-3.5-turbo-16k"
      | "gpt-3.5-turbo-0613"
      | "gpt-3.5-turbo-16k-0613";
    /**
     * A list of messages comprising the conversation so far. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb).
     *
     * @minItems 1
     */
    messages: [
      {
        /**
         * The role of the messages author. One of `system`, `user`, `assistant`, or `function`.
         */
        role: "system" | "user" | "assistant" | "function";
        /**
         * The contents of the message. `content` is required for all messages except assistant messages with function calls.
         */
        content?: string;
        /**
         * The name of the author of this message. `name` is required if role is `function`, and it should be the name of the function whose response is in the `content`. May contain a-z, A-Z, 0-9, and underscores, with a maximum length of 64 characters.
         */
        name?: string;
        /**
         * The name and arguments of a function that should be called, as generated by the model.
         */
        function_call?: {
          /**
           * The name of the function to call.
           */
          name?: string;
          /**
           * The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
           */
          arguments?: string;
        };
      },
      ...{
        /**
         * The role of the messages author. One of `system`, `user`, `assistant`, or `function`.
         */
        role: "system" | "user" | "assistant" | "function";
        /**
         * The contents of the message. `content` is required for all messages except assistant messages with function calls.
         */
        content?: string;
        /**
         * The name of the author of this message. `name` is required if role is `function`, and it should be the name of the function whose response is in the `content`. May contain a-z, A-Z, 0-9, and underscores, with a maximum length of 64 characters.
         */
        name?: string;
        /**
         * The name and arguments of a function that should be called, as generated by the model.
         */
        function_call?: {
          /**
           * The name of the function to call.
           */
          name?: string;
          /**
           * The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
           */
          arguments?: string;
        };
      }[]
    ];
    /**
     * A list of functions the model may generate JSON inputs for.
     *
     * @minItems 1
     */
    functions?: [
      {
        /**
         * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
         */
        name: string;
        /**
         * The description of what the function does.
         */
        description?: string;
        /**
         * The parameters the functions accepts, described as a JSON Schema object. See the [guide](/docs/guides/gpt/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format.
         */
        parameters?: {
          [k: string]: unknown;
        };
      },
      ...{
        /**
         * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
         */
        name: string;
        /**
         * The description of what the function does.
         */
        description?: string;
        /**
         * The parameters the functions accepts, described as a JSON Schema object. See the [guide](/docs/guides/gpt/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format.
         */
        parameters?: {
          [k: string]: unknown;
        };
      }[]
    ];
    /**
     * Controls how the model responds to function calls. "none" means the model does not call a function, and responds to the end-user. "auto" means the model can pick between an end-user or calling a function.  Specifying a particular function via `{"name":\ "my_function"}` forces the model to call that function. "none" is the default when no functions are present. "auto" is the default if functions are present.
     */
    function_call?:
      | ("none" | "auto")
      | {
          /**
           * The name of the function to call.
           */
          name: string;
        };
    /**
     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
     *
     * We generally recommend altering this or `top_p` but not both.
     *
     */
    temperature?: number;
    /**
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
     *
     * We generally recommend altering this or `temperature` but not both.
     *
     */
    top_p?: number;
    /**
     * How many chat completion choices to generate for each input message.
     */
    n?: number;
    /**
     * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).
     *
     */
    stream?: boolean;
    /**
     * Up to 4 sequences where the API will stop generating further tokens.
     *
     */
    stop?: string | [string] | [string, string] | [string, string, string] | [string, string, string, string];
    /**
     * The maximum number of [tokens](/tokenizer) to generate in the chat completion.
     *
     * The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) for counting tokens.
     *
     */
    max_tokens?: number;
    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
     *
     * [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)
     *
     */
    presence_penalty?: number;
    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
     *
     * [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)
     *
     */
    frequency_penalty?: number;
    /**
     * Modify the likelihood of specified tokens appearing in the completion.
     *
     * Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
     *
     */
    logit_bias?: {};
    /**
     * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
     *
     */
    user?: string;
  };
  "replicate/llama2"?: {
    model: "7b-chat" | "13b-chat" | "70b-chat";
    /**
     * System prompt to send to Llama v2. This is prepended to the prompt and helps guide system behavior.
     */
    system_prompt?: string;
    /**
     * Prompt to send to Llama v2.
     */
    prompt: string;
    /**
     * Maximum number of tokens to generate. A word is generally 2-3 tokens (minimum: 1)
     */
    max_new_tokens?: number;
    /**
     * Adjusts randomness of outputs, greater than 1 is random and 0 is deterministic, 0.75 is a good starting value. (minimum: 0.01; maximum: 5)
     */
    temperature?: number;
    /**
     * When decoding text, samples from the top p percentage of most likely tokens; lower to ignore less likely tokens (minimum: 0.01; maximum: 1)
     */
    top_p?: number;
    /**
     * Penalty for repeated words in generated text; 1 is no penalty, values greater than 1 discourage repetition, less than 1 encourage it. (minimum: 0.01; maximum: 5)
     */
    repetition_penalty?: number;
    /**
     * provide debugging output in logs
     */
    debug?: boolean;
  };
  anthropic?: {
    /**
     * The model that will complete your prompt.
     * As we improve Claude, we develop new versions of it that you can query.
     * This parameter controls which version of Claude answers your request.
     * Right now we are offering two model families: Claude, and Claude Instant.
     * You can use them by setting model to "claude-2" or "claude-instant-1", respectively.
     * See models for additional details.
     *
     */
    model: "claude-2" | "claude-2.0" | "claude-instant-1" | "claude-instant-1.1";
    /**
     * The prompt that you want Claude to complete.
     *
     * For proper response generation you will need to format your prompt as follows:
     * \n\nHuman: ${userQuestion}\n\nAssistant:
     * See our comments on prompts for more context.
     *
     */
    prompt: string | string[] | [number, ...number[]] | [[number, ...number[]], ...[number, ...number[]][]];
    /**
     * The maximum number of tokens to generate before stopping.
     *
     * Note that our models may stop before reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate.
     *
     */
    max_tokens_to_sample: number;
    /**
     * Amount of randomness injected into the response.
     *
     * Defaults to 1. Ranges from 0 to 1. Use temp closer to 0 for analytical / multiple choice, and closer to 1 for creative and generative tasks.
     *
     */
    temperature?: number;
    /**
     * Use nucleus sampling.
     *
     * In nucleus sampling, we compute the cumulative distribution over all the options
     * for each subsequent token in decreasing probability order and cut it off once
     * it reaches a particular probability specified by top_p. You should either alter temperature or top_p, but not both.
     *
     */
    top_p?: number;
    /**
     * Only sample from the top K options for each subsequent token.
     *
     * Used to remove "long tail" low probability responses. Learn more technical details here.
     *
     */
    top_k?: number;
    /**
     * Whether to incrementally stream the response using server-sent events.
     * See this guide to SSE events for details.type: boolean
     *
     */
    stream?: boolean;
    /**
     * Sequences that will cause the model to stop generating completion text.
     * Our models stop on "\n\nHuman:", and may include additional built-in stop sequences in the future. By providing the stop_sequences parameter, you may include additional strings that will cause the model to stop generating.
     *
     */
    stop_sequences?: string | [string] | [string, string] | [string, string, string] | [string, string, string, string];
    /**
     * An object describing metadata about the request.
     *
     */
    metadata?: {
      /**
       * An external identifier for the user who is associated with the request.
       *
       * This should be a uuid, hash value, or other opaque identifier. Anthropic may use this id to help detect abuse.
       * Do not include any identifying information such as name, email address, or phone number.
       *
       */
      user_id?: string;
    };
  };
}
